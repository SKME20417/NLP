{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bQKkoWxGM0zV"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3PpUwvSOCrl",
        "outputId": "b5b3729d-51d0-461e-b9da-21a8923d3f25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"
      ],
      "metadata": {
        "id": "TjN27KDwOHds"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.dep_, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qQEixyaOPqw",
        "outputId": "d122dd2a-01ef-4fe3-89f6-a9470bba742f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple nsubj PROPN\n",
            "is aux AUX\n",
            "looking ROOT VERB\n",
            "at prep ADP\n",
            "buying pcomp VERB\n",
            "U.K. dobj PROPN\n",
            "startup dobj NOUN\n",
            "for prep ADP\n",
            "$ quantmod SYM\n",
            "1 compound NUM\n",
            "billion pobj NUM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WnvGEOfOcpA",
        "outputId": "9dc7b2c1-1d99-44e0-e1f5-c6585dce75bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc5he6U7PN5F",
        "outputId": "c91c2ec5-ce2e-44a4-b5d2-45188f4c056c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coronavirus Coronavirus PROPN NNP nsubj Xxxxx True False\n",
            ": : PUNCT : punct : False False\n",
            "Delhi Delhi PROPN NNP compound Xxxxx True False\n",
            "resident resident NOUN NN nsubj xxxx True False\n",
            "tests test VERB VBZ appos xxxx True False\n",
            "positive positive ADJ JJ amod xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "coronavirus coronavirus NOUN NN pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "total total ADJ JJ ROOT xxxx True False\n",
            "31 31 NUM CD nummod dd False False\n",
            "people people NOUN NNS dobj xxxx True False\n",
            "infected infect VERB VBN acl xxxx True False\n",
            "in in ADP IN prep xx True True\n",
            "India India PROPN NNP pobj Xxxxx True False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Google, Apple crack down on fake coronavirus apps\")\n",
        "displacy.serve(doc, style=\"dep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jYbu3ZJQlAD",
        "outputId": "d00d9240-6ef4-4d01-d4ef-87283d27179c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o28nG29OQtrB",
        "outputId": "6b48b156-214a-42dc-8d0d-5615d7acd2aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delhi 13 18 GPE\n",
            "31 66 68 CARDINAL\n",
            "India 88 93 GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "text = \"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.serve(doc, style=\"ent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiXyCWT3RJGt",
        "outputId": "d31c7145-eff3-49f8-de5e-b85bdadd5646"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'ent' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9jqnqJeR0Oa",
        "outputId": "d870a17b-9a4e-4edd-e26d-79cd80d60726"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-20 08:06:38.694989: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6INIljmCSc-P",
        "outputId": "90ed3255-f2fb-4d94-d503-e64667dc4329"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "tokens = nlp(\"lion bear apple banana fadsfdshds\")\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L1CAqrVS2go",
        "outputId": "c0071d0c-58d2-4267-90cd-fdb787af1df6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lion True 55.145737 False\n",
            "bear True 52.114674 False\n",
            "apple True 43.366478 False\n",
            "banana True 31.620354 False\n",
            "fadsfdshds False 0.0 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\") \n",
        "tokens = nlp(\"lion bear cow apple mango spinach\")\n",
        "\n",
        "for token11 in tokens:\n",
        "    for token13 in tokens:\n",
        "        print(token11.text, token13.text, token11.similarity(token13))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6i5OO6ZTBDX",
        "outputId": "cb6f8133-ad98-442b-d5c2-cc9561fc24cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lion lion 1.0\n",
            "lion bear 0.40031397342681885\n",
            "lion cow 0.4524093568325043\n",
            "lion apple 0.06742795556783676\n",
            "lion mango 0.18510110676288605\n",
            "lion spinach 0.06951922923326492\n",
            "bear lion 0.40031397342681885\n",
            "bear bear 1.0\n",
            "bear cow 0.2781473696231842\n",
            "bear apple 0.18584337830543518\n",
            "bear mango 0.14443379640579224\n",
            "bear spinach 0.0758492723107338\n",
            "cow lion 0.4524093568325043\n",
            "cow bear 0.2781473696231842\n",
            "cow cow 1.0\n",
            "cow apple 0.2575658857822418\n",
            "cow mango 0.26287969946861267\n",
            "cow spinach 0.261837899684906\n",
            "apple lion 0.06742795556783676\n",
            "apple bear 0.18584337830543518\n",
            "apple cow 0.2575658857822418\n",
            "apple apple 1.0\n",
            "apple mango 0.6305076479911804\n",
            "apple spinach 0.5129707455635071\n",
            "mango lion 0.18510110676288605\n",
            "mango bear 0.14443379640579224\n",
            "mango cow 0.26287969946861267\n",
            "mango apple 0.6305076479911804\n",
            "mango mango 1.0\n",
            "mango spinach 0.5483009219169617\n",
            "spinach lion 0.06951922923326492\n",
            "spinach bear 0.0758492723107338\n",
            "spinach cow 0.261837899684906\n",
            "spinach apple 0.5129707455635071\n",
            "spinach mango 0.5483009219169617\n",
            "spinach spinach 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(doc.vocab.strings[\"coffee\"])  \n",
        "print(doc.vocab.strings[3197928453018144401])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps5Mj17oThlI",
        "outputId": "f33fffb2-5c95-4070-caee-bfcefb633289"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3197928453018144401\n",
            "coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(doc.vocab.strings[\"coffee\"])  \n",
        "print(doc.vocab.strings[3197928453018144401])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J7E5sZ6UiyU",
        "outputId": "fd59e5e7-2ef9-4797-c36c-ae730e830cc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3197928453018144401\n",
            "coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love tea, over coffee!\")\n",
        "for word in doc:\n",
        "    lexeme = doc.vocab[word.text]\n",
        "    print(lexeme)\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vcw6_EJUqvM",
        "outputId": "1aa2a35e-5a6b-498d-bd22-8588264f48ed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<spacy.lexeme.Lexeme object at 0x7fc65de587d0>\n",
            "I 4690420944186131903 X I I True False True en\n",
            "<spacy.lexeme.Lexeme object at 0x7fc65237d5a0>\n",
            "love 3702023516439754181 xxxx l ove True False False en\n",
            "<spacy.lexeme.Lexeme object at 0x7fc65237d140>\n",
            "tea 6041671307218480733 xxx t tea True False False en\n",
            "<spacy.lexeme.Lexeme object at 0x7fc6482ce0a0>\n",
            ", 2593208677638477497 , , , False False False en\n",
            "<spacy.lexeme.Lexeme object at 0x7fc6482ce1e0>\n",
            "over 5456543204961066030 xxxx o ver True False False en\n",
            "<spacy.lexeme.Lexeme object at 0x7fc6482ce320>\n",
            "coffee 3197928453018144401 xxxx c fee True False False en\n",
            "<spacy.lexeme.Lexeme object at 0x7fc6482ce500>\n",
            "! 17494803046312582752 ! ! ! False False False en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love tea, over coffee\")  \n",
        "print(doc.vocab.strings[\"tea\"]) \n",
        "print(doc.vocab.strings[6041671307218480733])   \n",
        "\n",
        "empty_doc = Doc(Vocab()) \n",
        "\n",
        "empty_doc.vocab.strings.add(\"tea\")  \n",
        "print(empty_doc.vocab.strings[6041671307218480733])\n",
        "\n",
        "new_doc = Doc(doc.vocab) \n",
        "print(new_doc.vocab.strings[6041671307218480733])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCT6INKuVBy-",
        "outputId": "dfdd10d6-1ae6-4ba9-dd63-caaceb18be2f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6041671307218480733\n",
            "tea\n",
            "tea\n",
            "tea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.kb import KnowledgeBase\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "\n",
        "\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "\n",
        "\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "kb.add_alias(alias=\"Douglas Adams\", entities=[\"Q42\"], probabilities=[0.9])\n",
        "\n",
        "print()\n",
        "print(\"Number of entities in KB:\",kb.get_size_entities()) \n",
        "print(\"Number of aliases in KB:\", kb.get_size_aliases())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6zA30fiVnDS",
        "outputId": "531e28a1-315c-4198-d593-35e876677bbc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of entities in KB: 3\n",
            "Number of aliases in KB: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.kb import KnowledgeBase\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "\n",
        "\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "\n",
        "candidates = kb.get_alias_candidates(\"Douglas\")\n",
        "for c in candidates:\n",
        "    print(\" \", c.entity_, c.prior_prob, c.entity_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSzsG2FoWIhT",
        "outputId": "c6e34e12-5d25-4436-9e92-e78d7a9e30d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Q1004791 0.6000000238418579 [0.0, 3.0, 5.0]\n",
            "  Q42 0.10000000149011612 [1.0, 9.0, -3.0]\n",
            "  Q5301561 0.20000000298023224 [-2.0, 4.0, 2.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rnn"
      ],
      "metadata": {
        "id": "bt2KBfNwWYrl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from collections import  namedtuple\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "zIQGUKLksP0z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/anna.txt', 'r') as f:\n",
        "    text=f.read()\n",
        "vocab = set(text)\n",
        "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
      ],
      "metadata": {
        "id": "MuI5bYF-sUzi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wPdyf2-6sWun",
        "outputId": "4db3bf6d-899e-4d27-981d-ebe33d77fc7d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTQrD6rEsdo2",
        "outputId": "8295b810-a616-4d4a-ce9c-0dff3954a869"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([34, 17, 58, 27, 12, 66, 13,  9, 50, 56, 56, 56,  2, 58, 27, 27,  3,\n",
              "        9, 38, 58,  1, 67, 82, 67, 66,  0,  9, 58, 13, 66,  9, 58, 82, 82,\n",
              "        9, 58, 82, 67, 11, 66, 41,  9, 66, 30, 66, 13,  3,  9, 59, 55, 17,\n",
              "       58, 27, 27,  3,  9, 38, 58,  1, 67, 82,  3,  9, 67,  0,  9, 59, 55,\n",
              "       17, 58, 27, 27,  3,  9, 67, 55,  9, 67, 12,  0,  9, 64, 39, 55, 56,\n",
              "       39, 58,  3, 52, 56, 56, 42, 30, 66, 13,  3, 12, 17, 67, 55],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtfkv71Ds9Ez",
        "outputId": "ed729507-1348-46f6-baa1-4f7938d1a079"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([34, 17, 58, 27, 12, 66, 13,  9, 50, 56, 56, 56,  2, 58, 27, 27,  3,\n",
              "        9, 38, 58,  1, 67, 82, 67, 66,  0,  9, 58, 13, 66,  9, 58, 82, 82,\n",
              "        9, 58, 82, 67, 11, 66, 41,  9, 66, 30, 66, 13,  3,  9, 59, 55, 17,\n",
              "       58, 27, 27,  3,  9, 38, 58,  1, 67, 82,  3,  9, 67,  0,  9, 59, 55,\n",
              "       17, 58, 27, 27,  3,  9, 67, 55,  9, 67, 12,  0,  9, 64, 39, 55, 56,\n",
              "       39, 58,  3, 52, 56, 56, 42, 30, 66, 13,  3, 12, 17, 67, 55],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.max(chars)+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrOZuqYAs-x6",
        "outputId": "6c7f3db3-5be3-4949-ba99-2d80835178b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
        "    \"\"\" \n",
        "    Split character data into training and validation sets, inputs and targets for each set.\n",
        "    \n",
        "    Arguments\n",
        "    ---------\n",
        "    chars: character array\n",
        "    batch_size: Size of examples in each of batch\n",
        "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
        "    split_frac: Fraction of batches to keep in the training set\n",
        "    \n",
        "    \n",
        "    Returns train_x, train_y, val_x, val_y\n",
        "    \"\"\"\n",
        "    \n",
        "    slice_size = batch_size * num_steps\n",
        "    n_batches = int(len(chars) / slice_size)\n",
        "    \n",
        "    # Drop the last few characters to make only full batches\n",
        "    x = chars[: n_batches*slice_size]\n",
        "    y = chars[1: n_batches*slice_size + 1]\n",
        "    \n",
        "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
        "    x = np.stack(np.split(x, batch_size))\n",
        "    y = np.stack(np.split(y, batch_size))\n",
        "    \n",
        "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
        "    \n",
        "    # Split into training and validation sets, keep the first split_frac batches for training\n",
        "    split_idx = int(n_batches*split_frac)\n",
        "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
        "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
        "    \n",
        "    return train_x, train_y, val_x, val_y"
      ],
      "metadata": {
        "id": "gBD7oxB5tAxl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
      ],
      "metadata": {
        "id": "bU_ICwHStDp0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XE5-FCItFtx",
        "outputId": "09e826b7-ebf6-4eac-ed17-205543eab171"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 178650)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x[:,:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPPtjsR8tHfw",
        "outputId": "e2e4ffb2-5c9e-4bed-fe92-873f94f37556"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[34, 17, 58, 27, 12, 66, 13,  9, 50, 56, 56, 56,  2, 58, 27, 27,\n",
              "         3,  9, 38, 58,  1, 67, 82, 67, 66,  0,  9, 58, 13, 66,  9, 58,\n",
              "        82, 82,  9, 58, 82, 67, 11, 66, 41,  9, 66, 30, 66, 13,  3,  9,\n",
              "        59, 55],\n",
              "       [ 9, 58,  1,  9, 55, 64, 12,  9, 14, 64, 67, 55, 14,  9, 12, 64,\n",
              "         9,  0, 12, 58,  3, 72, 69,  9, 58, 55,  0, 39, 66, 13, 66, 49,\n",
              "         9, 43, 55, 55, 58, 72,  9,  0,  1, 67, 82, 67, 55, 14, 72,  9,\n",
              "        81, 59],\n",
              "       [30, 67, 55, 52, 56, 56, 69, 10, 66,  0, 72,  9, 67, 12, 80,  0,\n",
              "         9,  0, 66, 12, 12, 82, 66, 49, 52,  9, 28, 17, 66,  9, 27, 13,\n",
              "        67, 31, 66,  9, 67,  0,  9,  1, 58, 14, 55, 67, 38, 67, 31, 66,\n",
              "        55, 12],\n",
              "       [55,  9, 49, 59, 13, 67, 55, 14,  9, 17, 67,  0,  9, 31, 64, 55,\n",
              "        30, 66, 13,  0, 58, 12, 67, 64, 55,  9, 39, 67, 12, 17,  9, 17,\n",
              "        67,  0, 56, 81, 13, 64, 12, 17, 66, 13,  9, 39, 58,  0,  9, 12,\n",
              "        17, 67],\n",
              "       [ 9, 67, 12,  9, 67,  0, 72,  9,  0, 67, 13, 68, 69,  9,  0, 58,\n",
              "        67, 49,  9, 12, 17, 66,  9, 64, 82, 49,  9,  1, 58, 55, 72,  9,\n",
              "        14, 66, 12, 12, 67, 55, 14,  9, 59, 27, 72,  9, 58, 55, 49, 56,\n",
              "        31, 13],\n",
              "       [ 9, 78, 12,  9, 39, 58,  0, 56, 64, 55, 82,  3,  9, 39, 17, 66,\n",
              "        55,  9, 12, 17, 66,  9,  0, 58,  1, 66,  9, 66, 30, 66, 55, 67,\n",
              "        55, 14,  9, 17, 66,  9, 31, 58,  1, 66,  9, 12, 64,  9, 12, 17,\n",
              "        66, 67],\n",
              "       [17, 66, 55,  9, 31, 64,  1, 66,  9, 38, 64, 13,  9,  1, 66, 72,\n",
              "        69,  9,  0, 17, 66,  9,  0, 58, 67, 49, 72,  9, 58, 55, 49,  9,\n",
              "        39, 66, 55, 12,  9, 81, 58, 31, 11,  9, 67, 55, 12, 64,  9, 12,\n",
              "        17, 66],\n",
              "       [41,  9, 81, 59, 12,  9, 55, 64, 39,  9,  0, 17, 66,  9, 39, 64,\n",
              "        59, 82, 49,  9, 13, 66, 58, 49, 67, 82,  3,  9, 17, 58, 30, 66,\n",
              "         9,  0, 58, 31, 13, 67, 38, 67, 31, 66, 49, 72,  9, 55, 64, 12,\n",
              "         9,  1],\n",
              "       [12,  9, 67,  0, 55, 80, 12, 52,  9, 28, 17, 66,  3, 80, 13, 66,\n",
              "         9, 27, 13, 64, 27, 13, 67, 66, 12, 64, 13,  0,  9, 64, 38,  9,\n",
              "        58,  9,  0, 64, 13, 12, 72, 56, 81, 59, 12,  9, 39, 66, 80, 13,\n",
              "        66,  9],\n",
              "       [ 9,  0, 58, 67, 49,  9, 12, 64,  9, 17, 66, 13,  0, 66, 82, 38,\n",
              "        72,  9, 58, 55, 49,  9, 81, 66, 14, 58, 55,  9, 58, 14, 58, 67,\n",
              "        55,  9, 38, 13, 64,  1,  9, 12, 17, 66,  9, 81, 66, 14, 67, 55,\n",
              "        55, 67]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(arrs, num_steps):\n",
        "    batch_size, slice_size = arrs[0].shape\n",
        "    \n",
        "    n_batches = int(slice_size/num_steps)\n",
        "    for b in range(n_batches):\n",
        "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
      ],
      "metadata": {
        "id": "NvVDg822tNbN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
        "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
        "    \n",
        "    # When we're using this network for sampling later, we'll be passing in\n",
        "    # one character at a time, so providing an option for that\n",
        "    if sampling == True:\n",
        "        batch_size, num_steps = 1, 1\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Declare placeholders we'll feed into the graph\n",
        "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
        "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
        "    \n",
        "    # Keep probability placeholder for drop out layers\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    # One-hot encoding the input and target characters\n",
        "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
        "    y_one_hot = tf.one_hot(targets, num_classes)\n",
        "\n",
        "    ### Build the RNN layers\n",
        "    # Use a basic LSTM cell\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "    \n",
        "    # Add dropout to the cell\n",
        "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "    \n",
        "    # Stack up multiple LSTM layers, for deep learning\n",
        "#     stacked_rnn = []\n",
        "#         for iiLyr in range():\n",
        "#             stacked_rnn.append(tf.nn.rnn_cell.LSTMCell(num_units=lstm_size, state_is_tuple=True))\n",
        "    \n",
        "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
        "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "    ### Run the data through the RNN layers\n",
        "    # This makes a list where each element is on step in the sequence\n",
        "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
        "    \n",
        "    # Run each sequence step through the RNN and collect the outputs\n",
        "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
        "    final_state = state\n",
        "    \n",
        "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
        "    seq_output = tf.concat(outputs, axis=1)\n",
        "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
        "    \n",
        "    # Now connect the RNN outputs to a softmax layer\n",
        "    with tf.variable_scope('softmax'):\n",
        "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
        "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
        "    \n",
        "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
        "    # of rows of logit outputs, one for each step and batch\n",
        "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
        "    \n",
        "    # Use softmax to get the probabilities for predicted characters\n",
        "    preds = tf.nn.softmax(logits, name='predictions')\n",
        "    \n",
        "    # Reshape the targets to match the logits\n",
        "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
        "    cost = tf.reduce_mean(loss)\n",
        "\n",
        "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
        "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
        "    \n",
        "    # Export the nodes\n",
        "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
        "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
        "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "    \n",
        "    return graph"
      ],
      "metadata": {
        "id": "eW8E9DfltN68"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8ecFsugAtQqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "num_steps = 100 \n",
        "lstm_size = 512\n",
        "num_layers = 2\n",
        "learning_rate = 0.001\n",
        "keep_prob = 10.5"
      ],
      "metadata": {
        "id": "Xuiy_y8ztRA9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir checkpoints"
      ],
      "metadata": {
        "id": "Vt7-9HostSzm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "# Save every N iterations\n",
        "save_every_n = 200\n",
        "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
        "\n",
        "model = build_rnn(len(vocab), \n",
        "                  batch_size=batch_size,\n",
        "                  num_steps=num_steps,\n",
        "                  learning_rate=learning_rate,\n",
        "                  lstm_size=lstm_size,\n",
        "                  num_layers=num_layers)\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "418HJwAitVTd",
        "outputId": "2c2bf3f8-042c-476f-f2e5-2de847f7555b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a867d0c419ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                   num_layers=num_layers)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-12c7c245c4a7>\u001b[0m in \u001b[0;36mbuild_rnn\u001b[0;34m(num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clip, sampling)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Declare placeholders we'll feed into the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "33uk0ZvdtXRJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}